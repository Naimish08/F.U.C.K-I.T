# -*- coding: utf-8 -*-
"""Multi Text Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10U8vccLi4o0s6nOwZUdcch1zMQhrdOPm

Set Up Google Colab Environment
"""

!pip cache purge

!pip uninstall -y transformers spacy nltk pandas numpy

# Install Python packages
!pip install transformers nltk pandas spacy spacy-experimental numpy

# Install spaCy models
!python -m spacy download en_core_web_sm
!python -m spacy download en_coreference_web_trf

# Download NLTK resources
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# Install spacy-experimental and en_coreference_web_trf
!pip install --force-reinstall spacy-experimental
!python -m spacy download en_coreference_web_trf --force

import transformers
import nltk
import pandas
import spacy
import numpy
import spacy_experimental  # Note: No separate version, tied to spacy
import pkg_resources
import os

# Print package versions
print("Transformers:", transformers.__version__)
print("NLTK:", nltk.__version__)
print("Pandas:", pandas.__version__)
print("spaCy:", spacy.__version__)
print("NumPy:", numpy.__version__)
# spacy-experimental version is tied to spacy; check if installed
try:
    spacy_experimental_version = pkg_resources.get_distribution("spacy-experimental").version
    print("spaCy-experimental:", spacy_experimental_version)
except pkg_resources.DistributionNotFound:
    print("spaCy-experimental: Not installed")

# Verify spaCy models
spacy_models = ["en_core_web_sm", "en_coreference_web_trf"]
for model in spacy_models:
    try:
        spacy.load(model)
        print(f"spaCy model {model}: Installed")
    except OSError:
        print(f"spaCy model {model}: Not installed")

# Verify NLTK resources
nltk_resources = ["punkt", "punkt_tab"]
for resource in nltk_resources:
    try:
        nltk.data.find(f"tokenizers/{resource}")
        print(f"NLTK resource {resource}: Installed")
    except LookupError:
        print(f"NLTK resource {resource}: Not installed")

!pip install spacy-transformers==1.3.5

import pandas as pd
characters = pd.read_csv("Characters.csv", sep=";")
potions = pd.read_csv("Potions.csv", sep=";")
spells = pd.read_csv("Spells.csv", sep=";")
print("Characters.csv columns:", characters.columns)
print("Potions.csv columns:", potions.columns)
print("Spells.csv columns:", spells.columns)

with open("NwOutput.txt", "r", encoding="utf-8") as file:
    sample = file.readlines()[:5]
    print("Sample of NwOutput_cleaned.txt:")
    for line in sample:
        print(line.strip())

from transformers import pipeline
import pandas as pd
import spacy
import json
import re
import numpy as np
from google.colab import files

# Custom JSON encoder to handle float32
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.floating):
            return float(obj)
        return super().default(obj)

# Load spaCy for POS tagging and dependency parsing
nlp = spacy.load("en_core_web_sm")

# Load cleaned text
input_file = "NwOutput.txt"
with open(input_file, "r", encoding="utf-8") as file:
    sentences = [line.strip() for line in file if line.strip()]

# Load CSV files
characters = pd.read_csv("Characters.csv", sep=";")
spells = pd.read_csv("Spells.csv", sep=";")
potions = pd.read_csv("Potions.csv", sep=";")

# Create reference sets (lowercase for matching)
character_names = set(characters["Name"].str.lower().str.strip())
spell_names = set(spells["Name"].str.lower().str.strip())  # Adjust to "Incantation" if needed
potion_names = set(potions["Name"].str.lower().str.strip())

# Initialize BERT-base NER pipeline
ner = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")

# Define interaction types
interaction_types = {
    "spells on": ["cast", "use", "perform"],
    "meets": ["meet", "see", "encounter"],
    "teaches": ["teach", "instruct", "explain"]
}

# Rule-based coreference resolution
def rule_based_coreference(sentences, current_idx, character_names):
    coref_entities = []
    context_window = sentences[max(0, current_idx-5):current_idx]  # Last 5 sentences
    last_male_character = None

    # Find the last male character in context
    for sentence in context_window[::-1]:  # Reverse to get most recent
        doc = nlp(sentence)
        ner_results = ner(sentence)
        person_entities = [e for e in ner_results if e["entity_group"] == "PER"]
        for entity in person_entities:
            entity_name = entity["word"].lower()
            if (entity_name in character_names or any(entity_name in name for name in character_names)) and "mr." in entity["word"].lower():
                last_male_character = entity["word"]
                break
        if last_male_character:
            break

    # Check current sentence for pronouns or nouns
    doc = nlp(sentences[current_idx])
    for token in doc:
        if token.lower_ == "he" or (token.pos_ == "NOUN" and token.lower_ == "man"):
            if last_male_character:
                coref_entities.append({
                    "word": token.text,
                    "resolved_to": last_male_character,
                    "type": "Coref"
                })

    return coref_entities

# Process sentences
output_list = []
situations = []
for idx, sentence in enumerate(sentences[:10]):  # Limit to 10 for testing
    # Run NER
    ner_results = ner(sentence)
    person_entities = [e for e in ner_results if e["entity_group"] == "PER"]

    # Validate characters
    validated_entities = []
    for entity in person_entities:
        entity_name = entity["word"].lower()
        if entity_name in character_names or any(entity_name in name for name in character_names):
            validated_entities.append({
                "word": entity["word"],
                "type": "Character",
                "score": float(entity["score"])
            })

    # Rule-based coreference
    coref_entities = rule_based_coreference(sentences, idx, character_names)

    # Combine NER and coreference entities
    all_entities = validated_entities + coref_entities

    # Check for spells and potions
    sentence_lower = sentence.lower()
    found_spells = [spell for spell in spell_names if spell in sentence_lower]
    found_potions = [potion for potion in potion_names if potion in sentence_lower]

    # Extract noun, verb, object, time
    doc = nlp(sentence)
    nouns = [token.text for token in doc if token.pos_ in ["NOUN", "PROPN"]]
    verbs = [token.lemma_.lower() for token in doc if token.pos_ == "VERB"]

    # Find direct object
    main_verb = next((token for token in doc if token.pos_ == "VERB"), None)
    direct_object = ""
    if main_verb:
        for child in main_verb.children:
            if child.dep_ == "dobj":
                direct_object = child.text
                break

    # Extract time
    time_info = ""
    time_patterns = [r"\b(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b",
                    r"\b(morning|afternoon|evening|night)\b",
                    r"\bat\s+\d{1,2}:\d{2}\b"]
    for pattern in time_patterns:
        match = re.search(pattern, sentence, re.IGNORECASE)
        if match:
            time_info = match.group()
            break

    # Generate situations
    characters = [e["word"] for e in validated_entities] + [e["resolved_to"] for e in coref_entities]
    characters = list(dict.fromkeys(characters))  # Remove duplicates
    if len(characters) >= 2:
        if found_spells:
            situation = f"{characters[0]} spells {found_spells[0]} on {characters[1]}"
            situations.append({"sentence": sentence, "situation": situation, "type": "spells on"})
        if any(verb in verbs for verb in interaction_types["meets"]) or not (found_spells or found_potions):
            situation = f"{characters[0]} meets {characters[1]}"
            situations.append({"sentence": sentence, "situation": situation, "type": "meets"})
        if any(verb in verbs for verb in interaction_types["teaches"]):
            situation = f"{characters[0]} teaches {characters[1]}"
            situations.append({"sentence": sentence, "situation": situation, "type": "teaches"})
    if found_potions and characters:
        situation = f"{characters[0]} uses potion {found_potions[0]}"
        situations.append({"sentence": sentence, "situation": situation, "type": "uses potion"})

    # Store output
    output_list.append({
        "sentence": sentence,
        "nouns": nouns[:5],
        "verbs": verbs[:5],
        "object": direct_object,
        "time": time_info,
        "entities": all_entities,
        "spells": found_spells,
        "potions": found_potions
    })

# Save outputs
with open("ner_nvo_coref_output.json", "w", encoding="utf-8") as file:
    json.dump(output_list, file, indent=2, ensure_ascii=False, cls=NumpyEncoder)
    files.download("ner_nvo_coref_output.json")

with open("situations.json", "w", encoding="utf-8") as file:
    json.dump(situations, file, indent=2, ensure_ascii=False, cls=NumpyEncoder)
    files.download("situations.json")

# Print sample results
for entry in output_list[:5]:
    print(f"Sentence: {entry['sentence']}")
    print(f"Nouns: {entry['nouns']}")
    print(f"Verbs: {entry['verbs']}")
    print(f"Object: {entry['object']}")
    print(f"Time: {entry['time']}")
    print(f"Entities: {entry['entities']}")
    print(f"Spells: {entry['spells']}")
    print(f"Potions: {entry['potions']}\n")

print("Processing complete. Results saved to 'ner_nvo_coref_output.json' and 'situations.json'.")